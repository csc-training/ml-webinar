\documentclass[aspectratio=1610,14pt]{beamer}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{minted}
\usepackage[absolute,overlay]{textpos}

\graphicspath{{images/}}

\usetheme{csctraining}

\usemintedstyle{emacs}
\definecolor{codebg}{rgb}{0.95,0.95,0.95}
\definecolor{codehl}{rgb}{0.95,0.95,0.05}
\setminted{highlightcolor=codehl,bgcolor=codebg}

\newcommand{\link}[1]{\alert{\url{#1}}}
\newcommand{\vitem}{\vfill\item}

\title{Machine Learning \\at CSC}
\subtitle{June 3, 2020\\[2mm]
  Mats Sj√∂berg -- mats.sjoberg@csc.fi}

\begin{document}

\begin{frame}{Overview}
  \tableofcontents
\end{frame}

\section{What CSC service to use?}

% TODO try with larger image and step-by-step display of each service
% with text apparing and disappearing - circle around place in picture

\begin{frame}{What CSC service to use?}
  \begin{itemize}
  \item CSC's supercomputer \alert{Puhti}
    \begin{itemize}
    \item<2-> Cluster with GPU-accelerated nodes
    \item<2-> Multi-user environment
    \end{itemize}
  \item Virtual server on \alert{Pouta}
    \begin{itemize}
    \item<3-> Your ``own'' server
    \item<3-> Less powerful than Puhti
    \end{itemize}
  \item Container cloud \alert{Rahti}
    \begin{itemize}
    \item<4-> Easy to run containers
    \item<4-> No GPUs \textbf{yet}
    \end{itemize}
  \end{itemize}
  \vspace{-5mm}
  \begin{flushright}
  \includegraphics[width=0.9\textwidth]{csc-services}  
  \end{flushright}
\end{frame}

\section{Puhti supercomputer}

\begin{frame}{Puhti supercomputer}
  \begin{columns}
    \begin{column}{0.76\linewidth}
      \begin{minipage}[c][0.6\textheight][s]{\columnwidth}
      \begin{itemize}
        \item \emph{Puhti-AI}, cluster with 80 nodes with \mbox{4 GPUs}
        each $\rightarrow$ 320 GPUs in total
        \vitem Latest generation Nvidia V100 GPUs (Volta) with 32 GB of memory
        \vitem Fast network: 2 $\times$ 100 Gbps links to each node
        \vitem Each node has a fast 3.2 TB local NVME disk
      \end{itemize}
      \vfill
      \end{minipage}
    \end{column}
    %% 
    \begin{column}{0.24\linewidth}
      \includegraphics[height=0.8\textheight]{puhti-long}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Getting access to Puhti}
  \link{https://docs.csc.fi/computing/overview/}

  \vspace{1em}

  To use Puhti you need to:
  \begin{itemize}
  \item Have a CSC account
  \item Be member of a CSC project, either by
    \begin{itemize}
    \item creating a new project, or
    \item joining an existing project (ask the PI to add you!)
    \end{itemize}
  \item Finally, the project needs to have Puhti access
  \end{itemize}

  \vspace{1em}

  $\rightarrow$ \quad MyCSC portal: \link{https://my.csc.fi/}

\end{frame}

\begin{frame}[fragile]{Accessing Puhti}
  \begin{itemize}
  \item Using an ssh client such as OpenSSH or PuTTY
  \item Basic Linux skills are required!
  \item More info: \link{https://docs.csc.fi/computing/connecting/}
  \end{itemize}

  \vspace{1em}

\begin{minted}{shell-session}
$ ssh <csc_username>@puhti.csc.fi
\end{minted}

\begin{minted}{shell-session}
$ ssh <csc_username>@puhti-login2.csc.fi
\end{minted}

\end{frame}

\begin{frame}{Supported frameworks}
  We currently support:
  \begin{itemize}
  \item \alert{Python Data} -- collection of Python libraries for data
    analytics and machine learning
  \item \alert{TensorFlow} -- deep learning library for Python
  \item \alert{PyTorch} -- machine learning framework for Python
  \item \alert{MXNet} -- deep learning library for Python
  \item \alert{RAPIDS} -- suite of libraries for data analytics and
    machine learning on GPUs
  \end{itemize}

  {\small \link{https://docs.csc.fi/apps/\#data-analytics-and-machine-learning}}
\end{frame}

\begin{frame}[fragile]{Example: TensorFlow}
  \begin{itemize}
  \vitem First check the application page for instructions:
    \link{https://docs.csc.fi/apps/tensorflow/}
  \vitem Load the default version:
\begin{verbatim}
module load tensorflow
\end{verbatim}
  \vitem or specific version:
\begin{verbatim}
module load tensorflow/2.0.0
\end{verbatim}
  \vitem \alert{Note:} some modules are \emph{Singularity-based}!
\end{itemize}
\vfill
\end{frame}

\begin{frame}[fragile]{What if some package is missing?}
  If you are using our module, but a trivial package is missing \ldots
  \begin{itemize}
  \vitem install it yourself, e.g.,
\begin{verbatim}
pip install --user <packagename>
\end{verbatim}
  \vitem \ldots or if it might be generally useful, send an email to \alert{\tt
      servicedesk@csc.fi} -- we can install it for you!
  \end{itemize}
  \vfill
  
\end{frame}

\begin{frame}[fragile]{What if some package is missing?}
  If you need a specific setup, and our modules are not right for you \ldots
  \begin{itemize}
  \vitem use a virtualenv:
\begin{minted}{shell-session}
$ python3 -m venv myenv
$ source myenv/bin/activate
$ pip install ...
\end{minted}

  \vitem use conda: {\small \link{https://docs.csc.fi/support/tutorials/conda/}}
  \vitem use singularity containers: {\small \link{https://docs.csc.fi/computing/containers/run-existing/}}
    
  \vitem or if generally useful, send an email to \alert{\tt
      servicedesk@csc.fi}
  \end{itemize}
  
\end{frame}

\begin{frame}{Running a job on Puhti}
  \alert{Don't run heavy computing jobs in the login nodes!}
  \vspace{2mm}
  \begin{itemize}
  \item Puhti uses the \emph{Slurm} batch job system
  \item Jobs do not run instantly but are put in a \emph{queue}
  \item Resources (runtime, memory, number of cores) need to be specified
  \end{itemize}

  \vspace{-4mm}
  \begin{center}
    \includegraphics[width=0.55\textwidth]{slurm1.png}    
  \end{center}
\end{frame}

\begin{frame}{Running a job on Puhti}
  \includegraphics[width=\textwidth]{slurm2.png}
\end{frame}

\begin{frame}[fragile]{Running a job on Puhti}

  Create a job script, for example {\tt run.sh}:
  
\begin{minted}[fontsize=\small,highlightlines={3,8}]{bash}
#!/bin/bash
#SBATCH --account=<project>
#SBATCH --partition=gpu
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=10
#SBATCH --mem=64G
#SBATCH --time=1:00:00
#SBATCH --gres=gpu:v100:1

module load tensorflow/2.0.0
srun python3 myprog.py <options>
\end{minted}

{\small \link{https://docs.csc.fi/computing/running/creating-job-scripts/}}
\end{frame}

\begin{frame}[fragile]{Running a job on Puhti}

  Example job script for Singularity-based modules:
  
\begin{minted}[fontsize=\small,highlightlines={11}]{bash}
#!/bin/bash
#SBATCH --account=<project>
#SBATCH --partition=gpu
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=10
#SBATCH --mem=64G
#SBATCH --time=1:00:00
#SBATCH --gres=gpu:v100:1

module load tensorflow/nvidia-20.03-tf2-py3
srun singularity_wrapper exec python3 myprog.py <options>
\end{minted}

\end{frame}

\begin{frame}[fragile]{Running a job on Puhti}
  Submit the job:
\begin{verbatim}
sbatch run.sh
\end{verbatim}
  \vfill
  
  Check the queue:
\begin{verbatim}
squeue -l -u $USER
\end{verbatim}
  \vfill
  
  Cancel a job:
\begin{verbatim}
scancel <jobid>
\end{verbatim}

  {\small \link{https://docs.csc.fi/computing/running/submitting-jobs/}}
\end{frame}

\section{Data storage}

\begin{frame}{Data storage on Puhti}

  \begin{itemize}
  \item Disk space and \emph{number of files} are limited on Puhti! \\
    {\small $\rightarrow$ We want to ensure that the shared (Lustre) filesystem works
    efficiently for everyone!}
  \item Useful command: {\tt csc-workspaces}
  \end{itemize}

  \vfill

  {\footnotesize
    \begin{tabular}{lllrrl}
             & Owner    & Path                      & Capacity & Number of files & Cleaning \\
    \hline
    home     & Personal & {\tt /users/<user-name>}  & 10 GiB   & 100 000 files   & No \\
    projappl & Project  & {\tt /projappl/<project>} & 50 GiB   & 100 000 files   & No \\
    scratch  & Project  & {\tt /scratch/<project>}  & 1 TiB    & 1 000 000 files & Yes - 90 days \\
    \end{tabular}
  }
  \vfill
  {\small
    Data quotas can be increased via MyCSC!

    \link{https://docs.csc.fi/computing/disk/}}
\end{frame}

\begin{frame}[fragile]{Using Allas}
  \begin{itemize}
  \item store big datasets in Allas, CSC's object storage
  \item download them to project scratch prior to computation
  \item you can also upload trained models (or keep in projappl)
  \end{itemize}

  \vfill
  
  % \begin{verbatim}
\begin{minted}{shell-session}
$ module load allas
$ allas-conf
$ cd /scratch/<your-project>
$ swift download <bucket-name> your-dataset.tar  
\end{minted}
%\end{verbatim}
\end{frame}

\begin{frame}{Large number of files}
  \begin{itemize}
  \vitem Many datasets contain a large number of small files
  \vitem Shared filesystem (Lustre) performs poorly in this scenario \\
    $\rightarrow$ noticable slowdowns for all Puhti users!
  \end{itemize}

  \vfill
  Consider alternatives:

  \begin{itemize}
  \vitem packaging your dataset into larger files                     
  \vitem use NVME fast local storage on GPU nodes
  \end{itemize}
\end{frame}

\begin{frame}{Using more efficient data formats}

  Instead of many small files, use one or a few bigger files.

  \vfill
  
  Examples:

  \begin{itemize}
  \vitem TensorFlow's TFRecord format
  \vitem HDF5
  \vitem LMDB
  \vitem ZIP, for example via Python's {\tt zipfile} library
  \end{itemize}

  \vfill
\end{frame}

\begin{frame}[fragile]{Fast local NVME drive}

  \begin{itemize}
  \item All GPU nodes have a local NVME drive
  \item Just add {\tt nvme:<number-of-GB>} to sbatch {\tt --gres} flag
  \end{itemize}

\begin{minted}[fontsize=\footnotesize,highlightlines={8,10,12}]{bash}
#!/bin/bash
#SBATCH --account=<project>
#SBATCH --partition=gpu
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=10
#SBATCH --mem=64G
#SBATCH --time=1:00:00
#SBATCH --gres=gpu:v100:1,nvme:100

tar xf /scratch/<your-project>/your-dataset.tar -C $LOCAL_SCRATCH

srun python3 myprog.py --data_dir=$LOCAL_SCRATCH <options>
\end{minted}

\end{frame}

\section{GPU utilization}

\begin{frame}{GPU utilization}

GPUs are an expensive resource compared to CPUs ($\times$ 60 BUs!) \\
$\rightarrow$ GPU should be maximally utilized!

\vspace{1em}

\begin{minipage}{0.8\textwidth}
For a running job:
\begin{itemize}
\item use {\tt squeue} to find out on what node (computer) it is running
\item ssh into that node, e.g., {\tt ssh r01g01}
\item find the process id of your job with \\
  {\tt ps -u \$USER }
\item run {\tt nvidia-smi}
\end{itemize}
\end{minipage}

\begin{textblock}{6}(9.8,8)
  \includegraphics[width=\textwidth]{v100}
\end{textblock}
\end{frame}

\begin{frame}{GPU utilization}

  For a finished job:

  \begin{itemize}
  \vitem run {\tt gpuseff <jobid>}
  \vitem shows GPU utilisation statistics for the whole running time
  \vitem \alert{note:} gpuseff is currently in testing usage, and still
    under development
  \end{itemize}

  \vfill
  
  You can always contact our service desk if you need advice on how to
  improve your GPU utilization!
\end{frame}

\begin{frame}[fragile]{Using multiple CPUs for ETL}
  \includegraphics[width=\textwidth]{etl}

  \begin{itemize}
  \item Common bottle-neck: CPU cannot keep up with GPU
  \item GPU has to wait for more data \ldots
  \item Solution: use more CPUs (they are ``cheaper'' than GPUs!)
  \end{itemize}

  \vspace{0.5em}
  
  A good \alert{rule of thumb in Puhti is to reserve 10 CPUs per GPU} \\
  (as there are 4 GPUs and 40 CPUs per node).

\end{frame}

\begin{frame}[fragile]{Using multiple CPUs for ETL}
  In Slurm scripts:
\begin{minted}{bash}
#SBATCH --cpus-per-task=10
\end{minted}

\vspace{0.5em}

\alert{Note:} using multiple CPUs \emph{not} automatic, code needs to support it!

\vspace{0.5em}

  For example in TensorFlow:

\begin{minted}[fontsize=\small]{python}
dataset = dataset.map(..., num_parallel_calls=10)
dataset = dataset.prefetch(buffer_size)
\end{minted}

  PyTorch:

\begin{minted}[fontsize=\small]{python}
train_loader = torch.utils.data.DataLoader(..., num_workers=10)
\end{minted}
\end{frame}

\section{Multi-GPU and multi-node jobs}

\begin{frame}[fragile]{Multi-GPU}

  Many frameworks support multi-GPU within a single node.

  \vspace{0.5em}
  
  Slurm script:
  \begin{minted}{bash}
#SBATCH --gres=gpu:v100:4
  \end{minted}

  \vspace{0.5em}
  
  \begin{columns}[t]
    \begin{column}{0.47\linewidth}
      TensorFlow:
      
      \begin{minted}[fontsize=\footnotesize]{python}
mirrored_strategy = \
  tf.distribute.MirroredStrategy()
with mirrored_strategy.scope():
    model = Sequential(...)
    model.add(...)
    model.add(...)
    model.compile(...)
      \end{minted}
    \end{column}
    \begin{column}{0.47\linewidth}
      PyTorch:
      
      \begin{minted}[fontsize=\footnotesize]{python}
model = MyModel(...)
if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)
model.to(device)
      \end{minted}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}[fragile]{Multi-GPU and multi-node}
  \begin{itemize}
  \item A single node has 4 GPUs
  \item If you need more than 4 GPUs, we recommend \alert{Horovod}
  \item Supported for TensorFlow and PyTorch on Puhti
  \item Uses MPI and NCCL for interprocess communication
  \item Modules with {\tt -hvd suffix}
  \end{itemize}

  \vspace{0.7em}

  Try: \vspace{-2mm}
\begin{verbatim}
module avail hvd
module load tensorflow/2.0.0-hvd
\end{verbatim}

\end{frame}  

\begin{frame}[fragile]{Slurm example for Horovod}
  Example slurm script that uses \alert{8 GPUs} across \alert{two computers}

\begin{itemize}
\item MPI terminology: 8 tasks, 2 nodes
\item Each task is 1 GPU and 10 CPUs
\end{itemize}
  
  \begin{minted}[fontsize=\small,highlightlines={4,5,6,8}]{bash}
#!/bin/bash
#SBATCH --account=<project>
#SBATCH --partition=gpu
#SBATCH --ntasks=8
#SBATCH --nodes=2
#SBATCH --cpus-per-task=10
#SBATCH --mem=32G
#SBATCH --time=1:00:00
#SBATCH --gres=gpu:v100:4

srun python3 myprog.py <options>
\end{minted}
\end{frame}

% srun --ntasks=$SLURM_NNODES --ntasks-per-node=1 \
%     tar xf /scratch/<your-project>/your-dataset.tar -C $LOCAL_SCRATCH  

\section{Singularity containers}

\begin{frame}[fragile]{Singularity on Puhti}
  \begin{itemize}
  \item Puhti supports Singularity-based containers
  \item Some of our modules use it -- just remember to prefix commands with \
    {\tt singularity\_wrapper exec}
  \item You can also convert your own Docker containers, see:
    {\small \link{https://docs.csc.fi/computing/containers/run-existing/}}
  \end{itemize}

  \vspace{5mm}
  Conversion (preferrably not in login nodes!):
  \vspace{-4mm}
\begin{minted}[fontsize=\small]{shell-session}
$ singularity build pytorch_20.03-py3.sif \
              docker://nvcr.io/nvidia/pytorch:20.03-py3
\end{minted}

Extract from Slurm script:
\vspace{-4mm}
\begin{minted}[fontsize=\small]{bash}
srun singularity_wrapper exec --nv pytorch_20.03-py3.sif \
    python3 myprog <options>
\end{minted}
  
\end{frame}

\begin{frame}[fragile]{Special Singularity-based applications}

  Specialized Singularity-based applications not shown with default modules

  \vspace{1em}

  \alert{Example:} Turku neural parser

\begin{minted}[fontsize=\small]{shell-session}
$ module use /appl/soft/ai/singularity/modulefiles/
$ module load turku-neural-parser/fi-en-sv-gpu
$ echo "Minulla on koira." | singularity_wrapper run \
    stream fi_tdt parse_plaintext
\end{minted}
  
\end{frame}
\end{document}

%%% Local Variables: 
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
